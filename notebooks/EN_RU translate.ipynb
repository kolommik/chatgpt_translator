{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "from langchain.schema import Document\n",
    "from langchain.document_transformers import DoctranTextTranslator\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import nest_asyncio\n",
    "\n",
    "a = load_dotenv(find_dotenv())  # read local .env file\n",
    "opena_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_content(content, model:str = \"gpt-3.5-turbo\") -> int:\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(content))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesize:\n",
      "42830 bytes\n",
      "599 words\n",
      "Number of tokens (GPT3.5): 9541\n"
     ]
    }
   ],
   "source": [
    "source_txt_filepath = \"../docs/EN_Toward an Ontology for Third Generation Systems Thinking.txt\"\n",
    "\n",
    "with open(source_txt_filepath, \"r\") as f:\n",
    "    source_text = f.read()\n",
    "\n",
    "filesize = len(source_text)\n",
    "wordssize = len(source_text.split(\"\\n\"))\n",
    "number_of_tokens = num_tokens_from_content(source_text)\n",
    "\n",
    "print(f\"Filesize:\\n{filesize} bytes\\n{wordssize} words\\nNumber of tokens (GPT3.5): {number_of_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split file into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits into 10 chunks\n",
      "{} - Chunk size: 10 bytes. Tokens (GPT3.5): 105\n",
      "{'Header 1': '1. Introduction'} - Chunk size: 26 bytes. Tokens (GPT3.5): 363\n",
      "{'Header 1': '2. Systems Ontology and Epistemology'} - Chunk size: 55 bytes. Tokens (GPT3.5): 815\n",
      "{'Header 1': '3. The first-generation approach'} - Chunk size: 49 bytes. Tokens (GPT3.5): 747\n",
      "{'Header 1': '4. The second-generation approach'} - Chunk size: 52 bytes. Tokens (GPT3.5): 839\n",
      "{'Header 1': '5. Continuous development'} - Chunk size: 73 bytes. Tokens (GPT3.5): 1136\n",
      "{'Header 1': '6. Scaleless descriptions of physical systems'} - Chunk size: 65 bytes. Tokens (GPT3.5): 998\n",
      "{'Header 1': '7. Constructivism'} - Chunk size: 97 bytes. Tokens (GPT3.5): 1496\n",
      "{'Header 1': '8. Thermodynamics of system evolution'} - Chunk size: 42 bytes. Tokens (GPT3.5): 664\n",
      "{'Header 1': '9. Conclusion'} - Chunk size: 121 bytes. Tokens (GPT3.5): 2300\n"
     ]
    }
   ],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    # (\"##\", \"Header 2\"),\n",
    "    # (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(source_text)\n",
    "\n",
    "print(f\"Splits into {len(md_header_splits)} chunks\")\n",
    "\n",
    "for item in md_header_splits:\n",
    "    size = len(item.page_content.split(\"\\n\"))\n",
    "    number_of_tokens = num_tokens_from_content(item.page_content)\n",
    "\n",
    "    print(f\"{item.metadata} - Chunk size: {size} bytes. Tokens (GPT3.5): {number_of_tokens}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# documents = [Document(page_content=source_text)]\n",
    "documents = [md_header_splits[0]]\n",
    "\n",
    "doc_translator = DoctranTextTranslator(\n",
    "        openai_api_key=opena_api_key, \n",
    "        language='russian',\n",
    "        # openai_api_model=\"gpt-3.5-turbo-16k\"\n",
    "        openai_api_model=\"gpt-3.5-turbo\"\n",
    "        )\n",
    "\n",
    "# translated_document = await doc_translator.atransform_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object Document can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\9-GitHubR\\gh_chatgpt_translator\\notebooks\\EN_RU translate.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/9-GitHubR/gh_chatgpt_translator/notebooks/EN_RU%20translate.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mawait\u001b[39;00m doc_translator\u001b[39m.\u001b[39matransform_documents(documents)\n",
      "File \u001b[1;32md:\\9-GitHubR\\gh_chatgpt_translator\\.venv\\lib\\site-packages\\langchain\\document_transformers\\doctran_text_translate.py:63\u001b[0m, in \u001b[0;36mDoctranTextTranslator.atransform_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m doctran_docs \u001b[39m=\u001b[39m [\n\u001b[0;32m     59\u001b[0m     doctran\u001b[39m.\u001b[39mparse(content\u001b[39m=\u001b[39mdoc\u001b[39m.\u001b[39mpage_content, metadata\u001b[39m=\u001b[39mdoc\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents\n\u001b[0;32m     61\u001b[0m ]\n\u001b[0;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(doctran_docs):\n\u001b[1;32m---> 63\u001b[0m     doctran_docs[i] \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m doc\u001b[39m.\u001b[39mtranslate(language\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage)\u001b[39m.\u001b[39mexecute()\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     65\u001b[0m     Document(page_content\u001b[39m=\u001b[39mdoc\u001b[39m.\u001b[39mtransformed_content, metadata\u001b[39m=\u001b[39mdoc\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m     66\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m doctran_docs\n\u001b[0;32m     67\u001b[0m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: object Document can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "await doc_translator.atransform_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translated_document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\9-GitHubR\\gh_chatgpt_translator\\notebooks\\EN_RU translate.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/9-GitHubR/gh_chatgpt_translator/notebooks/EN_RU%20translate.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m translated_document\n",
      "\u001b[1;31mNameError\u001b[0m: name 'translated_document' is not defined"
     ]
    }
   ],
   "source": [
    "translated_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_txt_filepath = \"../docs/RU_Toward an Ontology for Third Generation Systems Thinking.txt\"\n",
    "\n",
    "with open(dest_txt_filepath, \"w\") as f:\n",
    "    f.write(translated_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Filesize: 10 bytes\n",
      "Number of tokens (GPT3.5): 105\n",
      "{'Header 1': '1. Introduction'}\n",
      "Filesize: 26 bytes\n",
      "Number of tokens (GPT3.5): 363\n",
      "{'Header 1': '2. Systems Ontology and Epistemology'}\n",
      "Filesize: 55 bytes\n",
      "Number of tokens (GPT3.5): 815\n",
      "{'Header 1': '3. The first-generation approach'}\n",
      "Filesize: 49 bytes\n",
      "Number of tokens (GPT3.5): 747\n",
      "{'Header 1': '4. The second-generation approach'}\n",
      "Filesize: 52 bytes\n",
      "Number of tokens (GPT3.5): 839\n",
      "{'Header 1': '5. Continuous development'}\n",
      "Filesize: 73 bytes\n",
      "Number of tokens (GPT3.5): 1136\n",
      "{'Header 1': '6. Scaleless descriptions of physical systems'}\n",
      "Filesize: 65 bytes\n",
      "Number of tokens (GPT3.5): 998\n",
      "{'Header 1': '7. Constructivism'}\n",
      "Filesize: 97 bytes\n",
      "Number of tokens (GPT3.5): 1496\n",
      "{'Header 1': '8. Thermodynamics of system evolution'}\n",
      "Filesize: 42 bytes\n",
      "Number of tokens (GPT3.5): 664\n",
      "{'Header 1': '9. Conclusion'}\n",
      "Filesize: 121 bytes\n",
      "Number of tokens (GPT3.5): 2300\n"
     ]
    }
   ],
   "source": [
    "for item in md_header_splits:\n",
    "    filesize = len(item.page_content.split(\"\\n\"))\n",
    "    number_of_tokens = num_tokens_from_content(item.page_content)\n",
    "\n",
    "    print(item.metadata)\n",
    "    print(f\"Filesize: {filesize} bytes\\nNumber of tokens (GPT3.5): {number_of_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(md_header_splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = md_header_splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An ontology is a way to express what a subject area represents by defining\\na set of concepts and categories as well as how the concepts and categories are\\nrelated to each other. An ontology should indicate the objects that would be good\\nto distinguish in the world for reliable active/embodied inference within the\\nsubject area [2]. We formulate the task of creating an ontology in terms of\\nattention management, and вЂњexplicit formal specificationвЂќ which here indicates\\nthat these objects of attention are not spontaneously singled out but according to\\nsome explicit model as ontology specified by using another model as its formalism\\n(such as one of the foundational ontologies).\\nWe resolve the issue of creating an ontology according to Popperian\\nepistemology [3]: objects in an ontology appear by guessing, the acceptability of\\nthese guesses for judgments about the world is questioned, but guesses that\\nsurvive criticism are вЂњtaken seriously.вЂќ Good guesses about how the world is made\\nup of systems (what is the ontology of the system) and judgments that these\\nguesses help change the world for the better can be taken from current literature.\\nSuppose we do not know their critique and cannot immediately suggest\\nfalsification of those conjectures by reasoning and/or experiment. In that case, we\\nconsider them our best theories (i.e., the state-of-the-art) about the world, take\\nthem seriously and then teach people (replicate memes of those theories) to make\\nthose conjectures about the important objects shared.\\nOf course, we do not just tell people the systems ontology. However, we\\nteach them to attend to abstract and physical objects in the world, described by\\nshared explicit and as formal as possible specification, i.e., ontology. Problems\\nemerging within an activity performed with this ontology as a theory of the world\\nmean that ontology is falsified, but this is simply a reason to correct the ontology\\nby correcting identified errors. Systems ontology thereby continuously evolves. It\\nrepresents at each point in time the best we know about systems at that point. We\\n[4] have implemented a curriculum in which several hundred people a week are\\nnow learning to identify in the world objects described/typed by systems\\nontology. The curriculum implements teaching students fully conscious at the\\nstart and then interiorized вЂњautomaticвЂќ conceptual guidance of studentsвЂ™ attention\\nto objects described/typed by systems ontology. The student is trained to:3\\nвЂў to direct oneвЂ™s own and other agentsвЂ™ attention to objects from an\\nexplicit specification (ontology), that is, to direct attention\\nconceptually rather than spontaneously (Ontological Modeling\\ncourse),\\nвЂў to hold conceptually guided attention on a wide variety of time\\nscales, including collective attention in such agents as a team and\\nwhole enterprise via leadership, or influencing on community or\\nsociety (Self-Collectedness course),\\nвЂў to direct attention to systems (Systems Thinking course),\\nвЂў to direct attention to the activities/practices that are carried out by\\nconstructor/enabling systems (Methodology course),\\nвЂў structure the activities and roles that are necessary to be held in\\nattention in the course of systems engineering projects (Systems\\nEngineering course),\\nвЂў to draw the attention of oneself and the team to the objects\\ndescribed by the systems ontology specialization for systemsconstructors such as organizations/businesses (Systems\\nManagement course).\\nTo teach students a state-of-the-art systems ontology, we have harmonized\\nseveral particular conceptualizations of the concept of systems (i.e., we\\nperformed a merge of partial systems ontologies), which after merging, represent\\ntogether an ontology of the third generation of systems thinking. As usual, each\\ngeneration of systems thinking incorporates all the previous generationвЂ™s\\nachievements but adds something new.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Header 1': '2. Systems Ontology and Epistemology'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
