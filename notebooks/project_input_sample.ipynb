{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея\n",
    "Парсим полный проект (нужные файлы), добавляем их в контекст. Запускаем запрос в GPT на исправление (что хотим получить).\n",
    "Можем сохранять идеи по форматированию, но это не обязательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import tiktoken\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "a = load_dotenv(find_dotenv())  # read local .env file\n",
    "opena_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_content(content: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a given content string using a specific encoding model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        The text content for which to calculate the number of tokens.\n",
    "    model : str, optional\n",
    "        The name of the model whose encoding is to be used for tokenization. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of tokens in the content as determined by the encoding of the specified model.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function uses the 'tiktoken' library to obtain the encoding specific to the provided model. It then \n",
    "    calculates and returns the number of tokens that the content would be split into according to that encoding. \n",
    "    In case of an unrecognized model, it defaults to the encoding for \"cl100k_base\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Assuming 'tiktoken' is a hypothetical library for tokenization\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(content))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пробегаемся по директориям\n",
    "Составляем список того, что будет лежать в папке, со всеми путями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def read_files(folder_path: str, target_extensions: List[str], always_include_files: List[str], excluded_directories: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively read files from a given directory and its subdirectories, \n",
    "    filtering by file extensions, including specified always include files, \n",
    "    and excluding specified directories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The root directory from which to start reading files.\n",
    "    target_extensions : list of str\n",
    "        List of file extensions to include in the search (e.g., ['.py', '.txt']).\n",
    "    always_include_files : list of str\n",
    "        List of file names to always include in the final output regardless of their extensions or directories.\n",
    "    excluded_directories : list of str\n",
    "        List of directory names to exclude from the search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of dictionaries, each dictionary contains 'path' (the relative path of the file), \n",
    "        'filename' (the name of the file), and 'content' (the content of the file).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function reads the content of each file that matches the target extensions or is listed in the \n",
    "    always include files, excluding files in any of the excluded directories. It's designed to be used \n",
    "    for text files as it reads contents into a string.\n",
    "\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    for subdir, dirs, files in os.walk(folder_path):\n",
    "        dirs[:] = [d for d in dirs if d not in excluded_directories]\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            if file.endswith(tuple(target_extensions)) or file in always_include_files:\n",
    "                with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "                    files_list.append({\n",
    "                        'path': os.path.relpath(full_path, folder_path),\n",
    "                        'filename': file,\n",
    "                        'content': f.read()\n",
    "                    })\n",
    "    return files_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['docker-compose.yml',\n",
       " 'Dockerfile',\n",
       " 'app\\\\__init__.py',\n",
       " 'app\\\\src\\\\main.py',\n",
       " 'app\\\\src\\\\__init__.py',\n",
       " 'app\\\\src\\\\utils\\\\db.py',\n",
       " 'app\\\\src\\\\utils\\\\image_handler.py',\n",
       " 'app\\\\src\\\\utils\\\\tg_client.py',\n",
       " 'app\\\\src\\\\utils\\\\__init__.py',\n",
       " 'app\\\\tests\\\\__init__.py',\n",
       " 'sql\\\\init.sql']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = 'D:/9-GitHubR/gh_telegram_scrapper'\n",
    "target_extensions = ['.py','.sql',]\n",
    "always_include_files = [\"Dockerfile\",\"docker-compose.yml\"]\n",
    "excluded_directories = ['.vscode','.venv','app_streamlit']\n",
    "\n",
    "file_dict = read_files(folder_path, target_extensions, always_include_files, excluded_directories)\n",
    "\n",
    "print(len(file_dict))\n",
    "[x['path'] for x in file_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем статистику по токенам, сколько чего будет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_file_data(files_data: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a list of file dictionaries, reading content from each and augmenting the dictionary\n",
    "    with the length of content, the number of words, and the number of tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    files_data : list of dict\n",
    "        A list of dictionaries, each containing 'path', 'filename', and 'content' keys.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        The same list of dictionaries, but each dictionary is augmented with 'length' (number of characters),\n",
    "        'words' (number of words), and 'tokens' (number of tokens using num_tokens_from_content) fields.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function assumes that the content of each file is text and uses the 'num_tokens_from_content' function\n",
    "    to calculate the number of tokens according to a specific model's encoding.\n",
    "\n",
    "    \"\"\"\n",
    "    for file_dict in files_data:\n",
    "        content = file_dict['content']\n",
    "        file_dict['length'] = len(content)  # Number of characters\n",
    "        file_dict['words'] = len(content.split())  # Number of words\n",
    "        file_dict['tokens'] = num_tokens_from_content(content)  # Number of tokens\n",
    "\n",
    "    return files_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('docker-compose.yml', 302),\n",
       " ('Dockerfile', 85),\n",
       " ('app\\\\__init__.py', 0),\n",
       " ('app\\\\src\\\\main.py', 1062),\n",
       " ('app\\\\src\\\\__init__.py', 0),\n",
       " ('app\\\\src\\\\utils\\\\db.py', 740),\n",
       " ('app\\\\src\\\\utils\\\\image_handler.py', 73),\n",
       " ('app\\\\src\\\\utils\\\\tg_client.py', 624),\n",
       " ('app\\\\src\\\\utils\\\\__init__.py', 0),\n",
       " ('app\\\\tests\\\\__init__.py', 0),\n",
       " ('sql\\\\init.sql', 338)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_file_dict = augment_file_data(file_dict)\n",
    "\n",
    "print (\"Total tokens:\", sum([x[\"tokens\"] for x in stat_file_dict]))\n",
    "[(x[\"path\"], x[\"tokens\"]) for x in stat_file_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Грузим контент всего проекта в промпт, и задаем вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 7157\n",
      "Prompt Tokens: 6332\n",
      "Completion Tokens: 825\n",
      "Total Cost (USD): $0.0\n",
      "========================================\n",
      "Для улучшения обработки ошибок и разделения логики, мы можем разбить большой блок `try` на несколько меньших, каждый из которых будет отвечать за свою часть логики. Это позволит нам более точно управлять поведением программы в случае возникновения исключений и не прерывать обработку всех сообщений из-за ошибки в одном из них.\n",
      "\n",
      "Вот пример того, как можно разделить блок `try` в вашем коде:\n",
      "\n",
      "```python\n",
      "for dialog in dialogs:\n",
      "    dialog_type: str = dialog.get(\"_\", \" \")\n",
      "    dialog_id: int = dialog.get(\"id\", 0)\n",
      "    dialog_title: str = dialog.get(\"title\", \" \")\n",
      "\n",
      "    try:\n",
      "        logger.debug(f\"Processing channel: {dialog_title}\")\n",
      "        self.database.check_and_add_channel(\n",
      "            dialog_id, dialog_title, dialog_type\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error while adding channel {dialog_title}: {e}\")\n",
      "        continue\n",
      "\n",
      "    try:\n",
      "        logger.info(f\"Fetching messages from {dialog_title}...\")\n",
      "        last_message_id = self.database.get_last_message_id(dialog_id)\n",
      "        new_messages = await self.scrapper.get_new_dialog_messages(\n",
      "            dialog_id, offset_id=last_message_id, limit=10\n",
      "        )\n",
      "        self.database.add_messages(dialog_id, new_messages)\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error while fetching or saving messages from {dialog_title}: {e}\")\n",
      "        continue\n",
      "\n",
      "    for message in new_messages:\n",
      "        try:\n",
      "            attachments = await self.scrapper.get_message_attachments(message)\n",
      "            for attachment in attachments:\n",
      "                file_path = await self.attachment_handler.save_attachment(\n",
      "                    self.scrapper.client, attachment\n",
      "                )\n",
      "                if file_path:\n",
      "                    attachment_type_id = self.database.add_attachment_type(\n",
      "                        attachment[\"type\"]\n",
      "                    )\n",
      "                    self.database.add_attachment(\n",
      "                        attachment[\"id\"],\n",
      "                        message.id,\n",
      "                        dialog_id,\n",
      "                        attachment_type_id,\n",
      "                        file_path,\n",
      "                    )\n",
      "                    logger.debug(\n",
      "                        f\"Saved attachment for message {message.id} to {file_path}\"\n",
      "                    )\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error while processing attachments for message {message.id} in {dialog_title}: {e}\")\n",
      "\n",
      "        try:\n",
      "            replies = await self.scrapper.get_message_replies(\n",
      "                dialog_id, message.id, limit=10\n",
      "            )\n",
      "            last_reply_id = self.database.get_last_reply_id(dialog_id, message.id)\n",
      "            for reply in replies:\n",
      "                if reply[\"id\"] > last_reply_id:\n",
      "                    self.database.add_reply(\n",
      "                        main_dialog_id=dialog_id,\n",
      "                        reply_id=reply[\"id\"],\n",
      "                        main_message_id=message.id,\n",
      "                        reply_dialog_id=dialog_id,  # Так как reply в том же диалоге\n",
      "                        reply_to_msg_id=reply[\"reply_to_message_id\"],\n",
      "                        content=reply[\"content\"],\n",
      "                        sender=reply[\"sender\"],\n",
      "                        date=reply[\"date\"],\n",
      "                    )\n",
      "                    logger.debug(\n",
      "                        f\"Saved reply {reply['id']} for message {message.id}\"\n",
      "                    )\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error while processing replies for message {message.id} in {dialog_title}: {e}\")\n",
      "```\n",
      "\n",
      "Теперь каждый блок `try` обрабатывает свою часть логики: добавление канала, получение и сохранение сообщений, обработка вложений и сохранение ответов. Если в одном из этих блоков возникает ошибка, она логируется, и выполнение программы продолжается с следующего блока или сообщения, не прерывая всю обработку.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "folder_path = 'D:/9-GitHubR/gh_telegram_scrapper'\n",
    "target_extensions = ['.py','.sql',]\n",
    "always_include_files = [\"Dockerfile\",\"docker-compose.yml\"]\n",
    "excluded_directories = ['.vscode','.venv','app_streamlit']\n",
    "\n",
    "file_dict = read_files(folder_path, target_extensions, always_include_files, excluded_directories)\n",
    "\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content = item[\"content\"],\n",
    "        metadata={\"path\": item[\"path\"]}\n",
    "        )\n",
    "    for item in file_dict\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# old ------------------------------------\n",
    "# model_name = \"gpt-3.5-turbo-16k\"\n",
    "# current --------------------------------\n",
    "# Input costs || Output costs\n",
    "# model_name = \"gpt-3.5-turbo-1106\" # supports a 16K context | \t$0.0010 / 1K tokens ||\t$0.0020 / 1K tokens\n",
    "# model_name = \"gpt-3.5-turbo-instruct\" #  supports a 4K context | $0.0015 / 1K tokens ||\t$0.0020 / 1K tokens\n",
    "model_name = \"gpt-4-1106-preview\" # With 128k context, 4k - output | $0.01 / 1K tokens || $0.03 / 1K tokens\n",
    "# model_name = \"gpt-4\" # With 4k context |$0.03 / 1K tokens ||$0.06 / 1K tokens\n",
    "# model_name = \"gpt-4-32k\" # With 32k context |$0.06 / 1K tokens ||\t$0.12 / 1K tokens\n",
    "\n",
    "file_template_str = \"\"\"\n",
    "LOCAL FILEPATH: {path}\n",
    "CONTENTS:\n",
    "{page_content}\n",
    "\"\"\"\n",
    "\n",
    "doc_prompt = PromptTemplate.from_template(file_template_str)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "question = \"\"\"\n",
    "Поправь пожалуйста в текущем проекте в блоке main. Сейчас в одном блоке try сразу несколько функций объединено, в частности, когда \"пробегаемся по всем сообщениям\", то у нас идет: \n",
    "* Сохраняем attachments\n",
    "* Обработка и сохранение ответов на сообщения\n",
    "и в случае ошибки например с сохранением reply у нас цикл сбрасывается на следующее сообщение.\n",
    "\n",
    "Но по сути это 2 разные логические штуки, возможно стоит их по отдельности обрабатывать.\n",
    "\n",
    "Может быть вообще этот try разбить на несколько\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Поправь их пожалуйста. От полноты и точности твоего ответа зависит моя карьера. Я дам тебе премию в 100 USD в случае успешного исправления.\n",
    "# Опиши кратко структуру представленного проекта. Что проект делает? Из каких компонетов он состоит? Какие есть особенности (что сохраняет, куда именно)?\n",
    "\n",
    "# ----------------------------------------------------\n",
    "system_prompt = \"\"\"You are a high-end python and posgresql programmer.\n",
    "You don't teach how to write programs, you write them.\n",
    "Отвечай на русском языке.\"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(\n",
    "#     \"Context:\\n\\n{content}\\n\\nQuestion:\\n\\n{question}\\n\\n\"\n",
    "# )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"{system}\"),\n",
    "        (\"user\",\"Context:\\n\\n{content}\"),\n",
    "        (\"user\",\"Question:\\n\\n{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создание цепочки\n",
    "chain = (\n",
    "    {\n",
    "        # Создание контента из документов\n",
    "        \"content\": lambda docs: \"\\n\\n\".join(\n",
    "            format_document(doc, doc_prompt) for doc in docs\n",
    "        ),\n",
    "        # Добавление вопроса\n",
    "        \"question\": lambda _:question\n",
    "        # Добавление вопроса\n",
    "        ,\n",
    "        \"system\": lambda _:system_prompt\n",
    "    }\n",
    "    # | prompt_template  # Применение шаблона к контенту и вопросу\n",
    "    | chat_prompt_template  # Применение шаблона к контенту и вопросу\n",
    "    | ChatOpenAI(model_name=model_name, temperature=0.0, verbose=True)\n",
    "    | StrOutputParser()  # Преобразование вывода модели в строку\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# result = chain.invoke(docs)\n",
    "with get_openai_callback() as cb:\n",
    "    result = chain.invoke(docs, {\"callbacks\": [cb]})\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Total Tokens: {cb.total_tokens}\\n\"\\\n",
    "    f\"Prompt Tokens: {cb.prompt_tokens}\\n\"\\\n",
    "    f\"Completion Tokens: {cb.completion_tokens}\\n\"\\\n",
    "    f\"Total Cost (USD): ${cb.total_cost}\"\\\n",
    ")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# pprint.pprint(chain.invoke(docs).text)\n",
    "# print(chain.invoke(docs).text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос по конкретному файлу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 2284\n",
      "Prompt Tokens: 1621\n",
      "Completion Tokens: 663\n",
      "Total Cost (USD): $0.0\n",
      "========================================\n",
      "Чтобы добавить обработку других типов вложений (документы, видео и т.д.) в текущий проект, вам нужно будет расширить класс `Database` в модуле `db.py`, добавив методы для обработки и сохранения этих типов вложений. Вам также потребуется обновить схему базы данных, чтобы она могла хранить информацию о новых типах вложений.\n",
      "\n",
      "Ниже приведен пример того, как можно расширить класс `Database` для обработки документов и видео:\n",
      "\n",
      "```python\n",
      "class Database:\n",
      "    # ... существующие методы ...\n",
      "\n",
      "    def add_document_attachment(self, message_id: int, dialog_id: int, document):\n",
      "        \"\"\"Add a document attachment to the Attachments table.\n",
      "\n",
      "        Args:\n",
      "            message_id (int): The ID of the message the document belongs to.\n",
      "            dialog_id (int): The ID of the dialog the document belongs to.\n",
      "            document: The document object from Telethon.\n",
      "        \"\"\"\n",
      "        attachment_type_id = self.add_attachment_type('document')\n",
      "        file_path = document.file_name\n",
      "        attachment_id = document.id\n",
      "        self.add_attachment(attachment_id, message_id, dialog_id, attachment_type_id, file_path)\n",
      "\n",
      "    def add_video_attachment(self, message_id: int, dialog_id: int, video):\n",
      "        \"\"\"Add a video attachment to the Attachments table.\n",
      "\n",
      "        Args:\n",
      "            message_id (int): The ID of the message the video belongs to.\n",
      "            dialog_id (int): The ID of the dialog the video belongs to.\n",
      "            video: The video object from Telethon.\n",
      "        \"\"\"\n",
      "        attachment_type_id = self.add_attachment_type('video')\n",
      "        file_path = video.file_name\n",
      "        attachment_id = video.id\n",
      "        self.add_attachment(attachment_id, message_id, dialog_id, attachment_type_id, file_path)\n",
      "\n",
      "    # ... другие методы для обработки различных типов вложений ...\n",
      "```\n",
      "\n",
      "Вам также нужно будет обновить схему базы данных, чтобы добавить новые типы в таблицу `AttachmentTypes`, если они еще не добавлены.\n",
      "\n",
      "Кроме того, вам нужно будет обновить код, который извлекает сообщения и их вложения из Telethon, чтобы он мог распознавать и обрабатывать новые типы вложений. Это может включать в себя расширение логики, которая определяет тип вложения и вызывает соответствующий метод для его сохранения.\n",
      "\n",
      "Помните, что для полной интеграции новых типов вложений вам потребуется тщательно протестировать новый функционал, чтобы убедиться, что все работает корректно и что данные сохраняются в базу данных правильно.\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'D:/9-GitHubR/gh_telegram_scrapper'\n",
    "target_extensions = []\n",
    "always_include_files = [\"db.py\"]\n",
    "excluded_directories = ['.vscode','.venv','app_streamlit']\n",
    "\n",
    "file_dict = read_files(folder_path, target_extensions, always_include_files, excluded_directories)\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content = item[\"content\"],\n",
    "        metadata={\"path\": item[\"path\"]}\n",
    "        )\n",
    "    for item in file_dict\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# old ------------------------------------\n",
    "# model_name = \"gpt-3.5-turbo-16k\"\n",
    "# current --------------------------------\n",
    "# Input costs || Output costs\n",
    "# model_name = \"gpt-3.5-turbo-1106\" # supports a 16K context | \t$0.0010 / 1K tokens ||\t$0.0020 / 1K tokens\n",
    "# model_name = \"gpt-3.5-turbo-instruct\" #  supports a 4K context | $0.0015 / 1K tokens ||\t$0.0020 / 1K tokens\n",
    "model_name = \"gpt-4-1106-preview\" # With 128k context, 4k - output | $0.01 / 1K tokens || $0.03 / 1K tokens\n",
    "# model_name = \"gpt-4\" # With 4k context |$0.03 / 1K tokens ||$0.06 / 1K tokens\n",
    "# model_name = \"gpt-4-32k\" # With 32k context |$0.06 / 1K tokens ||\t$0.12 / 1K tokens\n",
    "\n",
    "file_template_str = \"\"\"\n",
    "LOCAL FILEPATH: {path}\n",
    "CONTENTS:\n",
    "{page_content}\n",
    "\"\"\"\n",
    "\n",
    "doc_prompt = PromptTemplate.from_template(file_template_str)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "question = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Добавь обработку исключений: В некоторых местах кода возможны исключительные ситуации, но они не обрабатываются.\n",
    "# Рекомендуется добавить соответствующие блоки try-except для обработки исключений и вывода информации об ошибках.\n",
    "# - В файле attachment_handler.py в методе `save_attachment` класса `AttachmentHandler` можно добавить обработку исключений, которые могут возникнуть при сохранении файлов, например `OSError`.\n",
    "\n",
    "\n",
    "# Опиши кратко структуру представленного проекта. Что проект делает? Из каких компонетов он состоит? Какие есть особенности (что сохраняет, куда именно)?\n",
    "\n",
    "# ----------------------------------------------------\n",
    "system_prompt = \"\"\"You are a high-end python and posgresql programmer.\n",
    "You don't teach how to write programs, you write them.\n",
    "Return full script (I don't have fingers).\n",
    "Return only full script.\n",
    "\"\"\"\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(\n",
    "#     \"Context:\\n\\n{content}\\n\\nQuestion:\\n\\n{question}\\n\\n\"\n",
    "# )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"{system}\"),\n",
    "        (\"user\",\"Context:\\n\\n{content}\"),\n",
    "        (\"user\",\"Question:\\n\\n{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Создание цепочки\n",
    "chain = (\n",
    "    {\n",
    "        # Создание контента из документов\n",
    "        \"content\": lambda docs: \"\\n\\n\".join(\n",
    "            format_document(doc, doc_prompt) for doc in docs\n",
    "        ),\n",
    "        # Добавление вопроса\n",
    "        \"question\": lambda _:question\n",
    "        # Добавление вопроса\n",
    "        ,\n",
    "        \"system\": lambda _:system_prompt\n",
    "    }\n",
    "    # | prompt_template  # Применение шаблона к контенту и вопросу\n",
    "    | chat_prompt_template  # Применение шаблона к контенту и вопросу\n",
    "    | ChatOpenAI(model_name=model_name, temperature=0.0, verbose=True)\n",
    "    | StrOutputParser()  # Преобразование вывода модели в строку\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# result = chain.invoke(docs)\n",
    "with get_openai_callback() as cb:\n",
    "    result = chain.invoke(docs, {\"callbacks\": [cb]})\n",
    "\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Total Tokens: {cb.total_tokens}\\n\"\\\n",
    "    f\"Prompt Tokens: {cb.prompt_tokens}\\n\"\\\n",
    "    f\"Completion Tokens: {cb.completion_tokens}\\n\"\\\n",
    "    f\"Total Cost (USD): ${cb.total_cost}\"\\\n",
    ")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# pprint.pprint(chain.invoke(docs).text)\n",
    "# print(chain.invoke(docs).text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
